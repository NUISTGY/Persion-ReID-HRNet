{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch._utils\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv2d') != -1:\n",
    "        init.kaiming_normal(m.weight.data, a=0, mode='fan_in')\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        init.kaiming_normal(m.weight.data, a=0, mode='fan_out')\n",
    "    elif classname.find('BatchNorm1d') != -1:\n",
    "        init.normal(m.weight.data, 1.0, 0.02)\n",
    "        init.constant(m.bias.data, 0.0)\n",
    "\n",
    "def weights_init_classifier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        init.normal(m.weight.data, std=0.001)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "# Defines the new fc layer and classification layer\n",
    "# |--Linear--|--bn--|--relu--|--Linear--|\n",
    "class ClassBlock(nn.Module):\n",
    "    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n",
    "        super(ClassBlock, self).__init__()\n",
    "        self.return_f = return_f\n",
    "        add_block = []\n",
    "        if linear:\n",
    "            add_block += [nn.Linear(input_dim, num_bottleneck)]\n",
    "        else:\n",
    "            num_bottleneck = input_dim\n",
    "        if bnorm:\n",
    "            add_block += [nn.BatchNorm1d(num_bottleneck)]\n",
    "        if relu:\n",
    "            add_block += [nn.LeakyReLU(0.1)]\n",
    "        if droprate>0:\n",
    "            add_block += [nn.Dropout(p=droprate)]\n",
    "        add_block = nn.Sequential(*add_block)\n",
    "        add_block.apply(weights_init_kaiming)\n",
    "\n",
    "        classifier = []\n",
    "        classifier += [nn.Linear(num_bottleneck, class_num)]\n",
    "        classifier = nn.Sequential(*classifier)\n",
    "        classifier.apply(weights_init_classifier)\n",
    "\n",
    "        self.add_block = add_block\n",
    "        self.classifier = classifier\n",
    "    def forward(self, x):\n",
    "        x = self.add_block(x)\n",
    "        if self.return_f:\n",
    "            f = x\n",
    "            x = self.classifier(x)\n",
    "            return x,f\n",
    "        else:\n",
    "            x = self.classifier(x)\n",
    "            return x\n",
    "#######################################################################        \n",
    "#空间注意力机制\n",
    "#-----------------------------------------\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "#######################################################################        \n",
    "#HRNet\n",
    "#-----------------------------------------\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
    "                               bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n",
    "                               momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n",
    "                 num_channels, fuse_method, multi_scale_output=True):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        self._check_branches(\n",
    "            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(\n",
    "            num_branches, blocks, num_blocks, num_channels)\n",
    "        self.fuse_layers = self._make_fuse_layers()\n",
    "        self.relu = nn.ReLU(False)\n",
    "\n",
    "    def _check_branches(self, num_branches, blocks, num_blocks,\n",
    "                        num_inchannels, num_channels):\n",
    "        if num_branches != len(num_blocks):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n",
    "                num_branches, len(num_blocks))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_channels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n",
    "                num_branches, len(num_channels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        if num_branches != len(num_inchannels):\n",
    "            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n",
    "                num_branches, len(num_inchannels))\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or \\\n",
    "           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index],\n",
    "                          num_channels[branch_index] * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(num_channels[branch_index] * block.expansion,\n",
    "                            momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.num_inchannels[branch_index],\n",
    "                            num_channels[branch_index], stride, downsample))\n",
    "        self.num_inchannels[branch_index] = \\\n",
    "            num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index],\n",
    "                                num_channels[branch_index]))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n",
    "        branches = []\n",
    "\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = []\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j],\n",
    "                                  num_inchannels[i],\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  0,\n",
    "                                  bias=False),\n",
    "                        nn.BatchNorm2d(num_inchannels[i], \n",
    "                                       momentum=BN_MOMENTUM),\n",
    "                        nn.Upsample(scale_factor=2**(j-i), mode='nearest')))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = []\n",
    "                    for k in range(i-j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                nn.BatchNorm2d(num_outchannels_conv3x3, \n",
    "                                            momentum=BN_MOMENTUM)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j],\n",
    "                                          num_outchannels_conv3x3,\n",
    "                                          3, 2, 1, bias=False),\n",
    "                                nn.BatchNorm2d(num_outchannels_conv3x3,\n",
    "                                            momentum=BN_MOMENTUM),\n",
    "                                nn.ReLU(False)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = []\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "blocks_dict = {\n",
    "    'BASIC': BasicBlock,\n",
    "    'BOTTLENECK': Bottleneck\n",
    "}\n",
    "\n",
    "\n",
    "class HighResolutionNet(nn.Module):\n",
    "\n",
    "    def __init__(self, cl, cfg, **kwargs):\n",
    "        super(HighResolutionNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.stage1_cfg = cfg['MODEL']['EXTRA']['STAGE1']\n",
    "        num_channels = self.stage1_cfg['NUM_CHANNELS'][0]\n",
    "        block = blocks_dict[self.stage1_cfg['BLOCK']]\n",
    "        num_blocks = self.stage1_cfg['NUM_BLOCKS'][0]\n",
    "        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n",
    "        stage1_out_channel = block.expansion*num_channels\n",
    "\n",
    "        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n",
    "        num_channels = self.stage2_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage2_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition1 = self._make_transition_layer(\n",
    "            [stage1_out_channel], num_channels)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(\n",
    "            self.stage2_cfg, num_channels)\n",
    "\n",
    "        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n",
    "        num_channels = self.stage3_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage3_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition2 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(\n",
    "            self.stage3_cfg, num_channels)\n",
    "\n",
    "        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n",
    "        num_channels = self.stage4_cfg['NUM_CHANNELS']\n",
    "        block = blocks_dict[self.stage4_cfg['BLOCK']]\n",
    "        num_channels = [\n",
    "            num_channels[i] * block.expansion for i in range(len(num_channels))]\n",
    "        self.transition3 = self._make_transition_layer(\n",
    "            pre_stage_channels, num_channels)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(\n",
    "            self.stage4_cfg, num_channels, multi_scale_output=True)\n",
    "\n",
    "        # Classification Head\n",
    "        self.incre_modules, self.downsamp_modules, \\\n",
    "            self.final_layer = self._make_head(pre_stage_channels)\n",
    "\n",
    "        self.classifier = nn.Linear(2048, 1000)\n",
    "        \n",
    "        #addition\n",
    "        self.SA1 = SpatialAttention()\n",
    "        self.SA2 = SpatialAttention()\n",
    "        self.avgpool_1 = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.maxpool_1 = nn.AdaptiveMaxPool2d((1,1))\n",
    "        self.avgpool_2 = nn.AdaptiveAvgPool2d((2,2))\n",
    "        self.maxpool_2 = nn.AdaptiveMaxPool2d((2,2))\n",
    "        self.avgpool_3 = nn.AdaptiveAvgPool2d((3,3))\n",
    "        self.maxpool_3 = nn.AdaptiveMaxPool2d((3,3))\n",
    "        self.avgpool_4 = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.maxpool_4 = nn.AdaptiveMaxPool2d((4,4))\n",
    "        self.classifier = ClassBlock(6144, 751, 0.5)\n",
    "        self.cl = cl\n",
    "        \n",
    "    def _make_head(self, pre_stage_channels):\n",
    "        head_block = Bottleneck\n",
    "        head_channels = [32, 64, 128, 256]\n",
    "\n",
    "        # Increasing the #channels on each resolution \n",
    "        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n",
    "        incre_modules = []\n",
    "        for i, channels  in enumerate(pre_stage_channels):\n",
    "            incre_module = self._make_layer(head_block,\n",
    "                                            channels,\n",
    "                                            head_channels[i],\n",
    "                                            1,\n",
    "                                            stride=1)\n",
    "            incre_modules.append(incre_module)\n",
    "        incre_modules = nn.ModuleList(incre_modules)\n",
    "            \n",
    "        # downsampling modules\n",
    "        downsamp_modules = []\n",
    "        for i in range(len(pre_stage_channels)-1):\n",
    "            in_channels = head_channels[i] * head_block.expansion\n",
    "            out_channels = head_channels[i+1] * head_block.expansion\n",
    "\n",
    "            downsamp_module = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=3,\n",
    "                          stride=2,\n",
    "                          padding=1),\n",
    "                nn.BatchNorm2d(out_channels, momentum=BN_MOMENTUM),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "            downsamp_modules.append(downsamp_module)\n",
    "        downsamp_modules = nn.ModuleList(downsamp_modules)\n",
    "\n",
    "        final_layer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=head_channels[3] * head_block.expansion,\n",
    "                out_channels=2048,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            ),\n",
    "            nn.BatchNorm2d(2048, momentum=BN_MOMENTUM),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        return incre_modules, downsamp_modules, final_layer\n",
    "\n",
    "    def _make_transition_layer(\n",
    "            self, num_channels_pre_layer, num_channels_cur_layer):\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = []\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i],\n",
    "                                  num_channels_cur_layer[i],\n",
    "                                  3,\n",
    "                                  1,\n",
    "                                  1,\n",
    "                                  bias=False),\n",
    "                        nn.BatchNorm2d(\n",
    "                            num_channels_cur_layer[i], momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                conv3x3s = []\n",
    "                for j in range(i+1-num_branches_pre):\n",
    "                    inchannels = num_channels_pre_layer[-1]\n",
    "                    outchannels = num_channels_cur_layer[i] \\\n",
    "                        if j == i-num_branches_pre else inchannels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            inchannels, outchannels, 3, 2, 1, bias=False),\n",
    "                        nn.BatchNorm2d(outchannels, momentum=BN_MOMENTUM),\n",
    "                        nn.ReLU(inplace=True)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, stride, downsample))\n",
    "        inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_stage(self, layer_config, num_inchannels,\n",
    "                    multi_scale_output=True):\n",
    "        num_modules = layer_config['NUM_MODULES']\n",
    "        num_branches = layer_config['NUM_BRANCHES']\n",
    "        num_blocks = layer_config['NUM_BLOCKS']\n",
    "        num_channels = layer_config['NUM_CHANNELS']\n",
    "        block = blocks_dict[layer_config['BLOCK']]\n",
    "        fuse_method = layer_config['FUSE_METHOD']\n",
    "\n",
    "        modules = []\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "\n",
    "            modules.append(\n",
    "                HighResolutionModule(num_branches,\n",
    "                                      block,\n",
    "                                      num_blocks,\n",
    "                                      num_inchannels,\n",
    "                                      num_channels,\n",
    "                                      fuse_method,\n",
    "                                      reset_multi_scale_output)\n",
    "            )\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        draw_features(8,8,x.cpu().numpy(),\"{}/conv1.png\".format(savepath))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.SA1(x) * x\n",
    "        draw_features(8,8,x.cpu().numpy(),\"{}/SA1.png\".format(savepath))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        draw_features(8,8,x.cpu().numpy(),\"{}/conv2.png\".format(savepath))\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.SA2(x) * x\n",
    "        draw_features(8,8,x.cpu().numpy(),\"{}/SA2.png\".format(savepath))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n",
    "            if self.transition1[i] is not None:\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list_s2 = self.stage2(x_list)\n",
    "        \n",
    "        x_list = []\n",
    "        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](y_list_s2[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list_s2[i])\n",
    "        y_list_s3 = self.stage3(x_list)\n",
    "        \n",
    "        x_list = []\n",
    "        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](y_list_s3[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list_s3[i])\n",
    "                \n",
    "        y_list = self.stage4(x_list)\n",
    "        \n",
    "        for i in range(4):\n",
    "            y_list[i] = self.incre_modules[i](y_list[i])\n",
    "        \n",
    "        draw_features(16,16,y_list[1].cpu().numpy(),\"{}/branch_out2.png\".format(savepath))\n",
    "        draw_features(32,32,y_list[3].cpu().numpy(),\"{}/branch_out4.png\".format(savepath))\n",
    "        \n",
    "        y_list[0] = self.avgpool_4(y_list[0]) + 0.5*self.maxpool_4(y_list[0])\n",
    "        y_list[1] = self.avgpool_2(y_list[1]) + 0.2*self.maxpool_2(y_list[1])\n",
    "        y_list[2] = self.avgpool_2(y_list[2]) + 0.2*self.maxpool_2(y_list[2])\n",
    "        y_list[3] = self.avgpool_1(y_list[3]) + 0.2*self.maxpool_1(y_list[3])\n",
    "        \n",
    "        y_list[0] = y_list[0].view((y_list[0].shape)[0],-1)\n",
    "        y_list[1] = y_list[1].view((y_list[1].shape)[0],-1)\n",
    "        y_list[2] = y_list[2].view((y_list[2].shape)[0],-1)\n",
    "        y_list[3] = y_list[3].view((y_list[3].shape)[0],-1)\n",
    "    \n",
    "    def init_weights(self, pretrained='',):\n",
    "        logger.info('=> init weights from normal distribution')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        if os.path.isfile(pretrained):\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
    "            model_dict = self.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                               if k in model_dict.keys()}\n",
    "            for k, _ in pretrained_dict.items():\n",
    "                logger.info(\n",
    "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
    "            model_dict.update(pretrained_dict)\n",
    "            self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_net(cl ,config, **kwargs):\n",
    "    model = HighResolutionNet(cl, config, **kwargs)\n",
    "    model.init_weights('hrnetv2_w32_imagenet_pretrained.pth')\n",
    "    return model\n",
    "# 网络初始化\n",
    "from config import config\n",
    "config.defrost()\n",
    "config.merge_from_file(r'cls_hrnet_w32_sgd_lr5e-2_wd1e-4_bs32_x100.yaml')\n",
    "config.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath='vis_ftnet/features_person'\n",
    "if not os.path.exists(savepath):\n",
    "    os.mkdir(savepath)\n",
    "\n",
    "\n",
    "def draw_features(width,height,x,savename):\n",
    "    tic=time.time()\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.05, hspace=0.05)\n",
    "    for i in range(width*height):\n",
    "        plt.subplot(height,width, i + 1)\n",
    "        plt.axis('off')\n",
    "        # plt.tight_layout()\n",
    "        img = x[0, i, :, :]\n",
    "        pmin = np.min(img)\n",
    "        pmax = np.max(img)\n",
    "        img = (img - pmin) / (pmax - pmin + 0.000001)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        print(\"{}/{}\".format(i,width*height))\n",
    "    fig.savefig(savename, dpi=100)\n",
    "    fig.clf()\n",
    "    plt.close()\n",
    "    print(\"time:{}\".format(time.time()-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:33: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:35: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:36: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:41: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/64\n",
      "1/64\n",
      "2/64\n",
      "3/64\n",
      "4/64\n",
      "5/64\n",
      "6/64\n",
      "7/64\n",
      "8/64\n",
      "9/64\n",
      "10/64\n",
      "11/64\n",
      "12/64\n",
      "13/64\n",
      "14/64\n",
      "15/64\n",
      "16/64\n",
      "17/64\n",
      "18/64\n",
      "19/64\n",
      "20/64\n",
      "21/64\n",
      "22/64\n",
      "23/64\n",
      "24/64\n",
      "25/64\n",
      "26/64\n",
      "27/64\n",
      "28/64\n",
      "29/64\n",
      "30/64\n",
      "31/64\n",
      "32/64\n",
      "33/64\n",
      "34/64\n",
      "35/64\n",
      "36/64\n",
      "37/64\n",
      "38/64\n",
      "39/64\n",
      "40/64\n",
      "41/64\n",
      "42/64\n",
      "43/64\n",
      "44/64\n",
      "45/64\n",
      "46/64\n",
      "47/64\n",
      "48/64\n",
      "49/64\n",
      "50/64\n",
      "51/64\n",
      "52/64\n",
      "53/64\n",
      "54/64\n",
      "55/64\n",
      "56/64\n",
      "57/64\n",
      "58/64\n",
      "59/64\n",
      "60/64\n",
      "61/64\n",
      "62/64\n",
      "63/64\n",
      "time:1.6667978763580322\n",
      "0/64\n",
      "1/64\n",
      "2/64\n",
      "3/64\n",
      "4/64\n",
      "5/64\n",
      "6/64\n",
      "7/64\n",
      "8/64\n",
      "9/64\n",
      "10/64\n",
      "11/64\n",
      "12/64\n",
      "13/64\n",
      "14/64\n",
      "15/64\n",
      "16/64\n",
      "17/64\n",
      "18/64\n",
      "19/64\n",
      "20/64\n",
      "21/64\n",
      "22/64\n",
      "23/64\n",
      "24/64\n",
      "25/64\n",
      "26/64\n",
      "27/64\n",
      "28/64\n",
      "29/64\n",
      "30/64\n",
      "31/64\n",
      "32/64\n",
      "33/64\n",
      "34/64\n",
      "35/64\n",
      "36/64\n",
      "37/64\n",
      "38/64\n",
      "39/64\n",
      "40/64\n",
      "41/64\n",
      "42/64\n",
      "43/64\n",
      "44/64\n",
      "45/64\n",
      "46/64\n",
      "47/64\n",
      "48/64\n",
      "49/64\n",
      "50/64\n",
      "51/64\n",
      "52/64\n",
      "53/64\n",
      "54/64\n",
      "55/64\n",
      "56/64\n",
      "57/64\n",
      "58/64\n",
      "59/64\n",
      "60/64\n",
      "61/64\n",
      "62/64\n",
      "63/64\n",
      "time:1.5852510929107666\n",
      "0/64\n",
      "1/64\n",
      "2/64\n",
      "3/64\n",
      "4/64\n",
      "5/64\n",
      "6/64\n",
      "7/64\n",
      "8/64\n",
      "9/64\n",
      "10/64\n",
      "11/64\n",
      "12/64\n",
      "13/64\n",
      "14/64\n",
      "15/64\n",
      "16/64\n",
      "17/64\n",
      "18/64\n",
      "19/64\n",
      "20/64\n",
      "21/64\n",
      "22/64\n",
      "23/64\n",
      "24/64\n",
      "25/64\n",
      "26/64\n",
      "27/64\n",
      "28/64\n",
      "29/64\n",
      "30/64\n",
      "31/64\n",
      "32/64\n",
      "33/64\n",
      "34/64\n",
      "35/64\n",
      "36/64\n",
      "37/64\n",
      "38/64\n",
      "39/64\n",
      "40/64\n",
      "41/64\n",
      "42/64\n",
      "43/64\n",
      "44/64\n",
      "45/64\n",
      "46/64\n",
      "47/64\n",
      "48/64\n",
      "49/64\n",
      "50/64\n",
      "51/64\n",
      "52/64\n",
      "53/64\n",
      "54/64\n",
      "55/64\n",
      "56/64\n",
      "57/64\n",
      "58/64\n",
      "59/64\n",
      "60/64\n",
      "61/64\n",
      "62/64\n",
      "63/64\n",
      "time:1.5601515769958496\n",
      "0/64\n",
      "1/64\n",
      "2/64\n",
      "3/64\n",
      "4/64\n",
      "5/64\n",
      "6/64\n",
      "7/64\n",
      "8/64\n",
      "9/64\n",
      "10/64\n",
      "11/64\n",
      "12/64\n",
      "13/64\n",
      "14/64\n",
      "15/64\n",
      "16/64\n",
      "17/64\n",
      "18/64\n",
      "19/64\n",
      "20/64\n",
      "21/64\n",
      "22/64\n",
      "23/64\n",
      "24/64\n",
      "25/64\n",
      "26/64\n",
      "27/64\n",
      "28/64\n",
      "29/64\n",
      "30/64\n",
      "31/64\n",
      "32/64\n",
      "33/64\n",
      "34/64\n",
      "35/64\n",
      "36/64\n",
      "37/64\n",
      "38/64\n",
      "39/64\n",
      "40/64\n",
      "41/64\n",
      "42/64\n",
      "43/64\n",
      "44/64\n",
      "45/64\n",
      "46/64\n",
      "47/64\n",
      "48/64\n",
      "49/64\n",
      "50/64\n",
      "51/64\n",
      "52/64\n",
      "53/64\n",
      "54/64\n",
      "55/64\n",
      "56/64\n",
      "57/64\n",
      "58/64\n",
      "59/64\n",
      "60/64\n",
      "61/64\n",
      "62/64\n",
      "63/64\n",
      "time:1.5881149768829346\n",
      "0/256\n",
      "1/256\n",
      "2/256\n",
      "3/256\n",
      "4/256\n",
      "5/256\n",
      "6/256\n",
      "7/256\n",
      "8/256\n",
      "9/256\n",
      "10/256\n",
      "11/256\n",
      "12/256\n",
      "13/256\n",
      "14/256\n",
      "15/256\n",
      "16/256\n",
      "17/256\n",
      "18/256\n",
      "19/256\n",
      "20/256\n",
      "21/256\n",
      "22/256\n",
      "23/256\n",
      "24/256\n",
      "25/256\n",
      "26/256\n",
      "27/256\n",
      "28/256\n",
      "29/256\n",
      "30/256\n",
      "31/256\n",
      "32/256\n",
      "33/256\n",
      "34/256\n",
      "35/256\n",
      "36/256\n",
      "37/256\n",
      "38/256\n",
      "39/256\n",
      "40/256\n",
      "41/256\n",
      "42/256\n",
      "43/256\n",
      "44/256\n",
      "45/256\n",
      "46/256\n",
      "47/256\n",
      "48/256\n",
      "49/256\n",
      "50/256\n",
      "51/256\n",
      "52/256\n",
      "53/256\n",
      "54/256\n",
      "55/256\n",
      "56/256\n",
      "57/256\n",
      "58/256\n",
      "59/256\n",
      "60/256\n",
      "61/256\n",
      "62/256\n",
      "63/256\n",
      "64/256\n",
      "65/256\n",
      "66/256\n",
      "67/256\n",
      "68/256\n",
      "69/256\n",
      "70/256\n",
      "71/256\n",
      "72/256\n",
      "73/256\n",
      "74/256\n",
      "75/256\n",
      "76/256\n",
      "77/256\n",
      "78/256\n",
      "79/256\n",
      "80/256\n",
      "81/256\n",
      "82/256\n",
      "83/256\n",
      "84/256\n",
      "85/256\n",
      "86/256\n",
      "87/256\n",
      "88/256\n",
      "89/256\n",
      "90/256\n",
      "91/256\n",
      "92/256\n",
      "93/256\n",
      "94/256\n",
      "95/256\n",
      "96/256\n",
      "97/256\n",
      "98/256\n",
      "99/256\n",
      "100/256\n",
      "101/256\n",
      "102/256\n",
      "103/256\n",
      "104/256\n",
      "105/256\n",
      "106/256\n",
      "107/256\n",
      "108/256\n",
      "109/256\n",
      "110/256\n",
      "111/256\n",
      "112/256\n",
      "113/256\n",
      "114/256\n",
      "115/256\n",
      "116/256\n",
      "117/256\n",
      "118/256\n",
      "119/256\n",
      "120/256\n",
      "121/256\n",
      "122/256\n",
      "123/256\n",
      "124/256\n",
      "125/256\n",
      "126/256\n",
      "127/256\n",
      "128/256\n",
      "129/256\n",
      "130/256\n",
      "131/256\n",
      "132/256\n",
      "133/256\n",
      "134/256\n",
      "135/256\n",
      "136/256\n",
      "137/256\n",
      "138/256\n",
      "139/256\n",
      "140/256\n",
      "141/256\n",
      "142/256\n",
      "143/256\n",
      "144/256\n",
      "145/256\n",
      "146/256\n",
      "147/256\n",
      "148/256\n",
      "149/256\n",
      "150/256\n",
      "151/256\n",
      "152/256\n",
      "153/256\n",
      "154/256\n",
      "155/256\n",
      "156/256\n",
      "157/256\n",
      "158/256\n",
      "159/256\n",
      "160/256\n",
      "161/256\n",
      "162/256\n",
      "163/256\n",
      "164/256\n",
      "165/256\n",
      "166/256\n",
      "167/256\n",
      "168/256\n",
      "169/256\n",
      "170/256\n",
      "171/256\n",
      "172/256\n",
      "173/256\n",
      "174/256\n",
      "175/256\n",
      "176/256\n",
      "177/256\n",
      "178/256\n",
      "179/256\n",
      "180/256\n",
      "181/256\n",
      "182/256\n",
      "183/256\n",
      "184/256\n",
      "185/256\n",
      "186/256\n",
      "187/256\n",
      "188/256\n",
      "189/256\n",
      "190/256\n",
      "191/256\n",
      "192/256\n",
      "193/256\n",
      "194/256\n",
      "195/256\n",
      "196/256\n",
      "197/256\n",
      "198/256\n",
      "199/256\n",
      "200/256\n",
      "201/256\n",
      "202/256\n",
      "203/256\n",
      "204/256\n",
      "205/256\n",
      "206/256\n",
      "207/256\n",
      "208/256\n",
      "209/256\n",
      "210/256\n",
      "211/256\n",
      "212/256\n",
      "213/256\n",
      "214/256\n",
      "215/256\n",
      "216/256\n",
      "217/256\n",
      "218/256\n",
      "219/256\n",
      "220/256\n",
      "221/256\n",
      "222/256\n",
      "223/256\n",
      "224/256\n",
      "225/256\n",
      "226/256\n",
      "227/256\n",
      "228/256\n",
      "229/256\n",
      "230/256\n",
      "231/256\n",
      "232/256\n",
      "233/256\n",
      "234/256\n",
      "235/256\n",
      "236/256\n",
      "237/256\n",
      "238/256\n",
      "239/256\n",
      "240/256\n",
      "241/256\n",
      "242/256\n",
      "243/256\n",
      "244/256\n",
      "245/256\n",
      "246/256\n",
      "247/256\n",
      "248/256\n",
      "249/256\n",
      "250/256\n",
      "251/256\n",
      "252/256\n",
      "253/256\n",
      "254/256\n",
      "255/256\n",
      "time:6.162579298019409\n",
      "0/1024\n",
      "1/1024\n",
      "2/1024\n",
      "3/1024\n",
      "4/1024\n",
      "5/1024\n",
      "6/1024\n",
      "7/1024\n",
      "8/1024\n",
      "9/1024\n",
      "10/1024\n",
      "11/1024\n",
      "12/1024\n",
      "13/1024\n",
      "14/1024\n",
      "15/1024\n",
      "16/1024\n",
      "17/1024\n",
      "18/1024\n",
      "19/1024\n",
      "20/1024\n",
      "21/1024\n",
      "22/1024\n",
      "23/1024\n",
      "24/1024\n",
      "25/1024\n",
      "26/1024\n",
      "27/1024\n",
      "28/1024\n",
      "29/1024\n",
      "30/1024\n",
      "31/1024\n",
      "32/1024\n",
      "33/1024\n",
      "34/1024\n",
      "35/1024\n",
      "36/1024\n",
      "37/1024\n",
      "38/1024\n",
      "39/1024\n",
      "40/1024\n",
      "41/1024\n",
      "42/1024\n",
      "43/1024\n",
      "44/1024\n",
      "45/1024\n",
      "46/1024\n",
      "47/1024\n",
      "48/1024\n",
      "49/1024\n",
      "50/1024\n",
      "51/1024\n",
      "52/1024\n",
      "53/1024\n",
      "54/1024\n",
      "55/1024\n",
      "56/1024\n",
      "57/1024\n",
      "58/1024\n",
      "59/1024\n",
      "60/1024\n",
      "61/1024\n",
      "62/1024\n",
      "63/1024\n",
      "64/1024\n",
      "65/1024\n",
      "66/1024\n",
      "67/1024\n",
      "68/1024\n",
      "69/1024\n",
      "70/1024\n",
      "71/1024\n",
      "72/1024\n",
      "73/1024\n",
      "74/1024\n",
      "75/1024\n",
      "76/1024\n",
      "77/1024\n",
      "78/1024\n",
      "79/1024\n",
      "80/1024\n",
      "81/1024\n",
      "82/1024\n",
      "83/1024\n",
      "84/1024\n",
      "85/1024\n",
      "86/1024\n",
      "87/1024\n",
      "88/1024\n",
      "89/1024\n",
      "90/1024\n",
      "91/1024\n",
      "92/1024\n",
      "93/1024\n",
      "94/1024\n",
      "95/1024\n",
      "96/1024\n",
      "97/1024\n",
      "98/1024\n",
      "99/1024\n",
      "100/1024\n",
      "101/1024\n",
      "102/1024\n",
      "103/1024\n",
      "104/1024\n",
      "105/1024\n",
      "106/1024\n",
      "107/1024\n",
      "108/1024\n",
      "109/1024\n",
      "110/1024\n",
      "111/1024\n",
      "112/1024\n",
      "113/1024\n",
      "114/1024\n",
      "115/1024\n",
      "116/1024\n",
      "117/1024\n",
      "118/1024\n",
      "119/1024\n",
      "120/1024\n",
      "121/1024\n",
      "122/1024\n",
      "123/1024\n",
      "124/1024\n",
      "125/1024\n",
      "126/1024\n",
      "127/1024\n",
      "128/1024\n",
      "129/1024\n",
      "130/1024\n",
      "131/1024\n",
      "132/1024\n",
      "133/1024\n",
      "134/1024\n",
      "135/1024\n",
      "136/1024\n",
      "137/1024\n",
      "138/1024\n",
      "139/1024\n",
      "140/1024\n",
      "141/1024\n",
      "142/1024\n",
      "143/1024\n",
      "144/1024\n",
      "145/1024\n",
      "146/1024\n",
      "147/1024\n",
      "148/1024\n",
      "149/1024\n",
      "150/1024\n",
      "151/1024\n",
      "152/1024\n",
      "153/1024\n",
      "154/1024\n",
      "155/1024\n",
      "156/1024\n",
      "157/1024\n",
      "158/1024\n",
      "159/1024\n",
      "160/1024\n",
      "161/1024\n",
      "162/1024\n",
      "163/1024\n",
      "164/1024\n",
      "165/1024\n",
      "166/1024\n",
      "167/1024\n",
      "168/1024\n",
      "169/1024\n",
      "170/1024\n",
      "171/1024\n",
      "172/1024\n",
      "173/1024\n",
      "174/1024\n",
      "175/1024\n",
      "176/1024\n",
      "177/1024\n",
      "178/1024\n",
      "179/1024\n",
      "180/1024\n",
      "181/1024\n",
      "182/1024\n",
      "183/1024\n",
      "184/1024\n",
      "185/1024\n",
      "186/1024\n",
      "187/1024\n",
      "188/1024\n",
      "189/1024\n",
      "190/1024\n",
      "191/1024\n",
      "192/1024\n",
      "193/1024\n",
      "194/1024\n",
      "195/1024\n",
      "196/1024\n",
      "197/1024\n",
      "198/1024\n",
      "199/1024\n",
      "200/1024\n",
      "201/1024\n",
      "202/1024\n",
      "203/1024\n",
      "204/1024\n",
      "205/1024\n",
      "206/1024\n",
      "207/1024\n",
      "208/1024\n",
      "209/1024\n",
      "210/1024\n",
      "211/1024\n",
      "212/1024\n",
      "213/1024\n",
      "214/1024\n",
      "215/1024\n",
      "216/1024\n",
      "217/1024\n",
      "218/1024\n",
      "219/1024\n",
      "220/1024\n",
      "221/1024\n",
      "222/1024\n",
      "223/1024\n",
      "224/1024\n",
      "225/1024\n",
      "226/1024\n",
      "227/1024\n",
      "228/1024\n",
      "229/1024\n",
      "230/1024\n",
      "231/1024\n",
      "232/1024\n",
      "233/1024\n",
      "234/1024\n",
      "235/1024\n",
      "236/1024\n",
      "237/1024\n",
      "238/1024\n",
      "239/1024\n",
      "240/1024\n",
      "241/1024\n",
      "242/1024\n",
      "243/1024\n",
      "244/1024\n",
      "245/1024\n",
      "246/1024\n",
      "247/1024\n",
      "248/1024\n",
      "249/1024\n",
      "250/1024\n",
      "251/1024\n",
      "252/1024\n",
      "253/1024\n",
      "254/1024\n",
      "255/1024\n",
      "256/1024\n",
      "257/1024\n",
      "258/1024\n",
      "259/1024\n",
      "260/1024\n",
      "261/1024\n",
      "262/1024\n",
      "263/1024\n",
      "264/1024\n",
      "265/1024\n",
      "266/1024\n",
      "267/1024\n",
      "268/1024\n",
      "269/1024\n",
      "270/1024\n",
      "271/1024\n",
      "272/1024\n",
      "273/1024\n",
      "274/1024\n",
      "275/1024\n",
      "276/1024\n",
      "277/1024\n",
      "278/1024\n",
      "279/1024\n",
      "280/1024\n",
      "281/1024\n",
      "282/1024\n",
      "283/1024\n",
      "284/1024\n",
      "285/1024\n",
      "286/1024\n",
      "287/1024\n",
      "288/1024\n",
      "289/1024\n",
      "290/1024\n",
      "291/1024\n",
      "292/1024\n",
      "293/1024\n",
      "294/1024\n",
      "295/1024\n",
      "296/1024\n",
      "297/1024\n",
      "298/1024\n",
      "299/1024\n",
      "300/1024\n",
      "301/1024\n",
      "302/1024\n",
      "303/1024\n",
      "304/1024\n",
      "305/1024\n",
      "306/1024\n",
      "307/1024\n",
      "308/1024\n",
      "309/1024\n",
      "310/1024\n",
      "311/1024\n",
      "312/1024\n",
      "313/1024\n",
      "314/1024\n",
      "315/1024\n",
      "316/1024\n",
      "317/1024\n",
      "318/1024\n",
      "319/1024\n",
      "320/1024\n",
      "321/1024\n",
      "322/1024\n",
      "323/1024\n",
      "324/1024\n",
      "325/1024\n",
      "326/1024\n",
      "327/1024\n",
      "328/1024\n",
      "329/1024\n",
      "330/1024\n",
      "331/1024\n",
      "332/1024\n",
      "333/1024\n",
      "334/1024\n",
      "335/1024\n",
      "336/1024\n",
      "337/1024\n",
      "338/1024\n",
      "339/1024\n",
      "340/1024\n",
      "341/1024\n",
      "342/1024\n",
      "343/1024\n",
      "344/1024\n",
      "345/1024\n",
      "346/1024\n",
      "347/1024\n",
      "348/1024\n",
      "349/1024\n",
      "350/1024\n",
      "351/1024\n",
      "352/1024\n",
      "353/1024\n",
      "354/1024\n",
      "355/1024\n",
      "356/1024\n",
      "357/1024\n",
      "358/1024\n",
      "359/1024\n",
      "360/1024\n",
      "361/1024\n",
      "362/1024\n",
      "363/1024\n",
      "364/1024\n",
      "365/1024\n",
      "366/1024\n",
      "367/1024\n",
      "368/1024\n",
      "369/1024\n",
      "370/1024\n",
      "371/1024\n",
      "372/1024\n",
      "373/1024\n",
      "374/1024\n",
      "375/1024\n",
      "376/1024\n",
      "377/1024\n",
      "378/1024\n",
      "379/1024\n",
      "380/1024\n",
      "381/1024\n",
      "382/1024\n",
      "383/1024\n",
      "384/1024\n",
      "385/1024\n",
      "386/1024\n",
      "387/1024\n",
      "388/1024\n",
      "389/1024\n",
      "390/1024\n",
      "391/1024\n",
      "392/1024\n",
      "393/1024\n",
      "394/1024\n",
      "395/1024\n",
      "396/1024\n",
      "397/1024\n",
      "398/1024\n",
      "399/1024\n",
      "400/1024\n",
      "401/1024\n",
      "402/1024\n",
      "403/1024\n",
      "404/1024\n",
      "405/1024\n",
      "406/1024\n",
      "407/1024\n",
      "408/1024\n",
      "409/1024\n",
      "410/1024\n",
      "411/1024\n",
      "412/1024\n",
      "413/1024\n",
      "414/1024\n",
      "415/1024\n",
      "416/1024\n",
      "417/1024\n",
      "418/1024\n",
      "419/1024\n",
      "420/1024\n",
      "421/1024\n",
      "422/1024\n",
      "423/1024\n",
      "424/1024\n",
      "425/1024\n",
      "426/1024\n",
      "427/1024\n",
      "428/1024\n",
      "429/1024\n",
      "430/1024\n",
      "431/1024\n",
      "432/1024\n",
      "433/1024\n",
      "434/1024\n",
      "435/1024\n",
      "436/1024\n",
      "437/1024\n",
      "438/1024\n",
      "439/1024\n",
      "440/1024\n",
      "441/1024\n",
      "442/1024\n",
      "443/1024\n",
      "444/1024\n",
      "445/1024\n",
      "446/1024\n",
      "447/1024\n",
      "448/1024\n",
      "449/1024\n",
      "450/1024\n",
      "451/1024\n",
      "452/1024\n",
      "453/1024\n",
      "454/1024\n",
      "455/1024\n",
      "456/1024\n",
      "457/1024\n",
      "458/1024\n",
      "459/1024\n",
      "460/1024\n",
      "461/1024\n",
      "462/1024\n",
      "463/1024\n",
      "464/1024\n",
      "465/1024\n",
      "466/1024\n",
      "467/1024\n",
      "468/1024\n",
      "469/1024\n",
      "470/1024\n",
      "471/1024\n",
      "472/1024\n",
      "473/1024\n",
      "474/1024\n",
      "475/1024\n",
      "476/1024\n",
      "477/1024\n",
      "478/1024\n",
      "479/1024\n",
      "480/1024\n",
      "481/1024\n",
      "482/1024\n",
      "483/1024\n",
      "484/1024\n",
      "485/1024\n",
      "486/1024\n",
      "487/1024\n",
      "488/1024\n",
      "489/1024\n",
      "490/1024\n",
      "491/1024\n",
      "492/1024\n",
      "493/1024\n",
      "494/1024\n",
      "495/1024\n",
      "496/1024\n",
      "497/1024\n",
      "498/1024\n",
      "499/1024\n",
      "500/1024\n",
      "501/1024\n",
      "502/1024\n",
      "503/1024\n",
      "504/1024\n",
      "505/1024\n",
      "506/1024\n",
      "507/1024\n",
      "508/1024\n",
      "509/1024\n",
      "510/1024\n",
      "511/1024\n",
      "512/1024\n",
      "513/1024\n",
      "514/1024\n",
      "515/1024\n",
      "516/1024\n",
      "517/1024\n",
      "518/1024\n",
      "519/1024\n",
      "520/1024\n",
      "521/1024\n",
      "522/1024\n",
      "523/1024\n",
      "524/1024\n",
      "525/1024\n",
      "526/1024\n",
      "527/1024\n",
      "528/1024\n",
      "529/1024\n",
      "530/1024\n",
      "531/1024\n",
      "532/1024\n",
      "533/1024\n",
      "534/1024\n",
      "535/1024\n",
      "536/1024\n",
      "537/1024\n",
      "538/1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539/1024\n",
      "540/1024\n",
      "541/1024\n",
      "542/1024\n",
      "543/1024\n",
      "544/1024\n",
      "545/1024\n",
      "546/1024\n",
      "547/1024\n",
      "548/1024\n",
      "549/1024\n",
      "550/1024\n",
      "551/1024\n",
      "552/1024\n",
      "553/1024\n",
      "554/1024\n",
      "555/1024\n",
      "556/1024\n",
      "557/1024\n",
      "558/1024\n",
      "559/1024\n",
      "560/1024\n",
      "561/1024\n",
      "562/1024\n",
      "563/1024\n",
      "564/1024\n",
      "565/1024\n",
      "566/1024\n",
      "567/1024\n",
      "568/1024\n",
      "569/1024\n",
      "570/1024\n",
      "571/1024\n",
      "572/1024\n",
      "573/1024\n",
      "574/1024\n",
      "575/1024\n",
      "576/1024\n",
      "577/1024\n",
      "578/1024\n",
      "579/1024\n",
      "580/1024\n",
      "581/1024\n",
      "582/1024\n",
      "583/1024\n",
      "584/1024\n",
      "585/1024\n",
      "586/1024\n",
      "587/1024\n",
      "588/1024\n",
      "589/1024\n",
      "590/1024\n",
      "591/1024\n",
      "592/1024\n",
      "593/1024\n",
      "594/1024\n",
      "595/1024\n",
      "596/1024\n",
      "597/1024\n",
      "598/1024\n",
      "599/1024\n",
      "600/1024\n",
      "601/1024\n",
      "602/1024\n",
      "603/1024\n",
      "604/1024\n",
      "605/1024\n",
      "606/1024\n",
      "607/1024\n",
      "608/1024\n",
      "609/1024\n",
      "610/1024\n",
      "611/1024\n",
      "612/1024\n",
      "613/1024\n",
      "614/1024\n",
      "615/1024\n",
      "616/1024\n",
      "617/1024\n",
      "618/1024\n",
      "619/1024\n",
      "620/1024\n",
      "621/1024\n",
      "622/1024\n",
      "623/1024\n",
      "624/1024\n",
      "625/1024\n",
      "626/1024\n",
      "627/1024\n",
      "628/1024\n",
      "629/1024\n",
      "630/1024\n",
      "631/1024\n",
      "632/1024\n",
      "633/1024\n",
      "634/1024\n",
      "635/1024\n",
      "636/1024\n",
      "637/1024\n",
      "638/1024\n",
      "639/1024\n",
      "640/1024\n",
      "641/1024\n",
      "642/1024\n",
      "643/1024\n",
      "644/1024\n",
      "645/1024\n",
      "646/1024\n",
      "647/1024\n",
      "648/1024\n",
      "649/1024\n",
      "650/1024\n",
      "651/1024\n",
      "652/1024\n",
      "653/1024\n",
      "654/1024\n",
      "655/1024\n",
      "656/1024\n",
      "657/1024\n",
      "658/1024\n",
      "659/1024\n",
      "660/1024\n",
      "661/1024\n",
      "662/1024\n",
      "663/1024\n",
      "664/1024\n",
      "665/1024\n",
      "666/1024\n",
      "667/1024\n",
      "668/1024\n",
      "669/1024\n",
      "670/1024\n",
      "671/1024\n",
      "672/1024\n",
      "673/1024\n",
      "674/1024\n",
      "675/1024\n",
      "676/1024\n",
      "677/1024\n",
      "678/1024\n",
      "679/1024\n",
      "680/1024\n",
      "681/1024\n",
      "682/1024\n",
      "683/1024\n",
      "684/1024\n",
      "685/1024\n",
      "686/1024\n",
      "687/1024\n",
      "688/1024\n",
      "689/1024\n",
      "690/1024\n",
      "691/1024\n",
      "692/1024\n",
      "693/1024\n",
      "694/1024\n",
      "695/1024\n",
      "696/1024\n",
      "697/1024\n",
      "698/1024\n",
      "699/1024\n",
      "700/1024\n",
      "701/1024\n",
      "702/1024\n",
      "703/1024\n",
      "704/1024\n",
      "705/1024\n",
      "706/1024\n",
      "707/1024\n",
      "708/1024\n",
      "709/1024\n",
      "710/1024\n",
      "711/1024\n",
      "712/1024\n",
      "713/1024\n",
      "714/1024\n",
      "715/1024\n",
      "716/1024\n",
      "717/1024\n",
      "718/1024\n",
      "719/1024\n",
      "720/1024\n",
      "721/1024\n",
      "722/1024\n",
      "723/1024\n",
      "724/1024\n",
      "725/1024\n",
      "726/1024\n",
      "727/1024\n",
      "728/1024\n",
      "729/1024\n",
      "730/1024\n",
      "731/1024\n",
      "732/1024\n",
      "733/1024\n",
      "734/1024\n",
      "735/1024\n",
      "736/1024\n",
      "737/1024\n",
      "738/1024\n",
      "739/1024\n",
      "740/1024\n",
      "741/1024\n",
      "742/1024\n",
      "743/1024\n",
      "744/1024\n",
      "745/1024\n",
      "746/1024\n",
      "747/1024\n",
      "748/1024\n",
      "749/1024\n",
      "750/1024\n",
      "751/1024\n",
      "752/1024\n",
      "753/1024\n",
      "754/1024\n",
      "755/1024\n",
      "756/1024\n",
      "757/1024\n",
      "758/1024\n",
      "759/1024\n",
      "760/1024\n",
      "761/1024\n",
      "762/1024\n",
      "763/1024\n",
      "764/1024\n",
      "765/1024\n",
      "766/1024\n",
      "767/1024\n",
      "768/1024\n",
      "769/1024\n",
      "770/1024\n",
      "771/1024\n",
      "772/1024\n",
      "773/1024\n",
      "774/1024\n",
      "775/1024\n",
      "776/1024\n",
      "777/1024\n",
      "778/1024\n",
      "779/1024\n",
      "780/1024\n",
      "781/1024\n",
      "782/1024\n",
      "783/1024\n",
      "784/1024\n",
      "785/1024\n",
      "786/1024\n",
      "787/1024\n",
      "788/1024\n",
      "789/1024\n",
      "790/1024\n",
      "791/1024\n",
      "792/1024\n",
      "793/1024\n",
      "794/1024\n",
      "795/1024\n",
      "796/1024\n",
      "797/1024\n",
      "798/1024\n",
      "799/1024\n",
      "800/1024\n",
      "801/1024\n",
      "802/1024\n",
      "803/1024\n",
      "804/1024\n",
      "805/1024\n",
      "806/1024\n",
      "807/1024\n",
      "808/1024\n",
      "809/1024\n",
      "810/1024\n",
      "811/1024\n",
      "812/1024\n",
      "813/1024\n",
      "814/1024\n",
      "815/1024\n",
      "816/1024\n",
      "817/1024\n",
      "818/1024\n",
      "819/1024\n",
      "820/1024\n",
      "821/1024\n",
      "822/1024\n",
      "823/1024\n",
      "824/1024\n",
      "825/1024\n",
      "826/1024\n",
      "827/1024\n",
      "828/1024\n",
      "829/1024\n",
      "830/1024\n",
      "831/1024\n",
      "832/1024\n",
      "833/1024\n",
      "834/1024\n",
      "835/1024\n",
      "836/1024\n",
      "837/1024\n",
      "838/1024\n",
      "839/1024\n",
      "840/1024\n",
      "841/1024\n",
      "842/1024\n",
      "843/1024\n",
      "844/1024\n",
      "845/1024\n",
      "846/1024\n",
      "847/1024\n",
      "848/1024\n",
      "849/1024\n",
      "850/1024\n",
      "851/1024\n",
      "852/1024\n",
      "853/1024\n",
      "854/1024\n",
      "855/1024\n",
      "856/1024\n",
      "857/1024\n",
      "858/1024\n",
      "859/1024\n",
      "860/1024\n",
      "861/1024\n",
      "862/1024\n",
      "863/1024\n",
      "864/1024\n",
      "865/1024\n",
      "866/1024\n",
      "867/1024\n",
      "868/1024\n",
      "869/1024\n",
      "870/1024\n",
      "871/1024\n",
      "872/1024\n",
      "873/1024\n",
      "874/1024\n",
      "875/1024\n",
      "876/1024\n",
      "877/1024\n",
      "878/1024\n",
      "879/1024\n",
      "880/1024\n",
      "881/1024\n",
      "882/1024\n",
      "883/1024\n",
      "884/1024\n",
      "885/1024\n",
      "886/1024\n",
      "887/1024\n",
      "888/1024\n",
      "889/1024\n",
      "890/1024\n",
      "891/1024\n",
      "892/1024\n",
      "893/1024\n",
      "894/1024\n",
      "895/1024\n",
      "896/1024\n",
      "897/1024\n",
      "898/1024\n",
      "899/1024\n",
      "900/1024\n",
      "901/1024\n",
      "902/1024\n",
      "903/1024\n",
      "904/1024\n",
      "905/1024\n",
      "906/1024\n",
      "907/1024\n",
      "908/1024\n",
      "909/1024\n",
      "910/1024\n",
      "911/1024\n",
      "912/1024\n",
      "913/1024\n",
      "914/1024\n",
      "915/1024\n",
      "916/1024\n",
      "917/1024\n",
      "918/1024\n",
      "919/1024\n",
      "920/1024\n",
      "921/1024\n",
      "922/1024\n",
      "923/1024\n",
      "924/1024\n",
      "925/1024\n",
      "926/1024\n",
      "927/1024\n",
      "928/1024\n",
      "929/1024\n",
      "930/1024\n",
      "931/1024\n",
      "932/1024\n",
      "933/1024\n",
      "934/1024\n",
      "935/1024\n",
      "936/1024\n",
      "937/1024\n",
      "938/1024\n",
      "939/1024\n",
      "940/1024\n",
      "941/1024\n",
      "942/1024\n",
      "943/1024\n",
      "944/1024\n",
      "945/1024\n",
      "946/1024\n",
      "947/1024\n",
      "948/1024\n",
      "949/1024\n",
      "950/1024\n",
      "951/1024\n",
      "952/1024\n",
      "953/1024\n",
      "954/1024\n",
      "955/1024\n",
      "956/1024\n",
      "957/1024\n",
      "958/1024\n",
      "959/1024\n",
      "960/1024\n",
      "961/1024\n",
      "962/1024\n",
      "963/1024\n",
      "964/1024\n",
      "965/1024\n",
      "966/1024\n",
      "967/1024\n",
      "968/1024\n",
      "969/1024\n",
      "970/1024\n",
      "971/1024\n",
      "972/1024\n",
      "973/1024\n",
      "974/1024\n",
      "975/1024\n",
      "976/1024\n",
      "977/1024\n",
      "978/1024\n",
      "979/1024\n",
      "980/1024\n",
      "981/1024\n",
      "982/1024\n",
      "983/1024\n",
      "984/1024\n",
      "985/1024\n",
      "986/1024\n",
      "987/1024\n",
      "988/1024\n",
      "989/1024\n",
      "990/1024\n",
      "991/1024\n",
      "992/1024\n",
      "993/1024\n",
      "994/1024\n",
      "995/1024\n",
      "996/1024\n",
      "997/1024\n",
      "998/1024\n",
      "999/1024\n",
      "1000/1024\n",
      "1001/1024\n",
      "1002/1024\n",
      "1003/1024\n",
      "1004/1024\n",
      "1005/1024\n",
      "1006/1024\n",
      "1007/1024\n",
      "1008/1024\n",
      "1009/1024\n",
      "1010/1024\n",
      "1011/1024\n",
      "1012/1024\n",
      "1013/1024\n",
      "1014/1024\n",
      "1015/1024\n",
      "1016/1024\n",
      "1017/1024\n",
      "1018/1024\n",
      "1019/1024\n",
      "1020/1024\n",
      "1021/1024\n",
      "1022/1024\n",
      "1023/1024\n",
      "time:30.99524736404419\n",
      "total time:46.127066135406494\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "load_epoch = 70 \n",
    "name = 'ft_net'\n",
    "\n",
    "def load_network(network):\n",
    "    save_path = os.path.join('./model',name,'net_%s.pth'%load_epoch)\n",
    "    network.load_state_dict(torch.load(save_path))\n",
    "    return network\n",
    "\n",
    "model=get_cls_net(False, config)\n",
    "#model = load_network(model)\n",
    "model=model.cuda()\n",
    "\n",
    "img=cv2.imread('person.jpg')\n",
    "img=cv2.resize(img,(128,256));\n",
    "img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "img=transform(img).cuda()\n",
    "img=img.unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    start=time.time()\n",
    "    out=model(img)\n",
    "    print(\"total time:{}\".format(time.time()-start))\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
